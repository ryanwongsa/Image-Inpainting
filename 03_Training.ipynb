{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from dataloaders.mask_generator import MaskGenerator\n",
    "from dataloaders.images_dataset import ImagesDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from models.pconv_unet import PConvUNet\n",
    "from models.vgg16_extractor import VGG16Extractor\n",
    "\n",
    "from loss.loss_compute import LossCompute\n",
    "\n",
    "from utils.preprocessing import Preprocessor\n",
    "\n",
    "from argparse import ArgumentParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = \"../../Repos/image-inpainting/dataset/train_0\"\n",
    "VALID_DIR = \"../../Repos/image-inpainting/dataset/test\"\n",
    "MASK_DIR = \"../../../Repos/image-inpainting/dataset/irregular_mask/irregular_mask/disocclusion_img_mask/\"\n",
    "\n",
    "HEIGHT, WIDTH = 256,256\n",
    "INVERT_MASK = False\n",
    "NUM_WORKERS = 0\n",
    "BS = 2\n",
    "LR  = 0.0002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSS_FACTORS = {\n",
    "    \"loss_hole\": 6.0, \n",
    "    \"loss_valid\": 1.0,\n",
    "    \"loss_perceptual\":  1.0, # 0.05,\n",
    "    \"loss_style_out\": 120.0,\n",
    "    \"loss_style_comp\": 120.0,\n",
    "    \"loss_tv\": 10.0  #0.1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HParams(object):\n",
    "    def __init__(self):\n",
    "        self.train_dir = TRAIN_DIR\n",
    "        self.valid_dir = VALID_DIR\n",
    "        self.mask_dir = MASK_DIR\n",
    "        \n",
    "        self.height = HEIGHT\n",
    "        self.width = WIDTH\n",
    "        self.invert_mask = INVERT_MASK\n",
    "        \n",
    "        self.num_workers = NUM_WORKERS\n",
    "        self.batch_size = BS\n",
    "        \n",
    "        self.learning_rate = LR\n",
    "        self.loss_factors = LOSS_FACTORS\n",
    "        \n",
    "hparams = HParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageInpaintingSystem(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, hparams):\n",
    "        super(ImageInpaintingSystem, self).__init__()\n",
    "        self.hparams = hparams\n",
    "        self.pConvUNet = PConvUNet()\n",
    "        \n",
    "        self.vgg16extractor = VGG16Extractor().to(\"cuda\")\n",
    "        for param in self.vgg16extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.lossCompute = LossCompute(self.vgg16extractor, device=\"cuda\")\n",
    "        \n",
    "        self.preprocess = Preprocessor(\"cuda\")\n",
    "\n",
    "    def forward(self, masked_img_tensor, mask_tensor):\n",
    "        return self.pConvUNet(masked_img_tensor, mask_tensor)\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        masked_img, mask, image  = batch\n",
    "        \n",
    "        img_tensor = self.preprocess.normalize(image.type(torch.float))\n",
    "        mask_tensor = mask.type(torch.float).transpose(1, 3)\n",
    "        masked_img_tensor = self.preprocess.normalize(masked_img.type(torch.float))\n",
    "        \n",
    "        ls_fn = self.lossCompute.loss_total(mask_tensor, self.hparams.loss_factors)\n",
    "        output = self.forward(masked_img_tensor, mask_tensor)\n",
    "        loss, dict_losses = ls_fn(img_tensor, output)\n",
    "\n",
    "        dict_losses_train = {}\n",
    "        for key, value in dict_losses.items():\n",
    "            dict_losses_train[key] = value.item()\n",
    "\n",
    "        self.logger.experiment.add_scalars('loss/train',dict_losses_train, self.global_step)\n",
    "        self.logger.experiment.add_scalars('loss/overview',{'train_loss': loss}, self.global_step)\n",
    "        \n",
    "        return {'loss': loss,'progress_bar': {'train_loss': loss}} #,  'log': {'train_loss': loss}}\n",
    "\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        masked_img, mask, image = batch\n",
    "        \n",
    "        img_tensor = self.preprocess.normalize(image.type(torch.float))\n",
    "        mask_tensor = mask.type(torch.float).transpose(1, 3)\n",
    "        masked_img_tensor = self.preprocess.normalize(masked_img.type(torch.float))\n",
    "        \n",
    "        ls_fn = self.lossCompute.loss_total(mask_tensor, self.hparams.loss_factors)\n",
    "        output = self.forward(masked_img_tensor, mask_tensor)\n",
    "        loss, dict_losses = ls_fn(img_tensor, output)\n",
    "        \n",
    "        psnr = self.lossCompute.PSNR(img_tensor, output)\n",
    "        if batch_nb == 0:\n",
    "            res = np.clip(self.preprocess.unnormalize(output).detach().cpu().numpy(),0,1)\n",
    "            original_img = np.clip(self.preprocess.unnormalize(masked_img_tensor).detach().cpu().numpy(),0,1)\n",
    "            combined_imgs = []\n",
    "            for i in range(image.shape[0]):\n",
    "                combined_img = np.concatenate((original_img[i], res[i], image[i].detach().cpu().numpy()), axis=1)\n",
    "                combined_imgs.append(combined_img)\n",
    "            combined_imgs = np.concatenate(combined_imgs)\n",
    "            self.logger.experiment.add_image('images', combined_imgs, dataformats='HWC') \n",
    "        dict_valid = {'val_loss': loss.mean(), 'psnr': psnr.mean(), **dict_losses}\n",
    "        \n",
    "        return dict_valid\n",
    "    \n",
    "    def validation_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        avg_psnr = torch.stack([x['psnr'] for x in outputs]).mean()\n",
    "        \n",
    "        avg_loss_hole = torch.stack([x['loss_hole'] for x in outputs]).mean()\n",
    "        avg_loss_valid = torch.stack([x['loss_valid'] for x in outputs]).mean()\n",
    "        avg_loss_perceptual = torch.stack([x['loss_perceptual'] for x in outputs]).mean()\n",
    "        avg_loss_style_out = torch.stack([x['loss_style_out'] for x in outputs]).mean()\n",
    "        avg_loss_style_comp = torch.stack([x['loss_style_comp'] for x in outputs]).mean()\n",
    "        avg_loss_tv = torch.stack([x['loss_tv'] for x in outputs]).mean()\n",
    "        valid_dict = {\n",
    "            \"loss_hole\": avg_loss_hole, \n",
    "            \"loss_valid\": avg_loss_valid,\n",
    "            \"loss_perceptual\": avg_loss_perceptual,\n",
    "            \"loss_style_out\": avg_loss_style_out,\n",
    "            \"loss_style_comp\": avg_loss_style_comp,\n",
    "            \"loss_tv\": avg_loss_tv\n",
    "        }\n",
    "\n",
    "        self.logger.experiment.add_scalars('loss/valid',valid_dict, self.global_step)\n",
    "        self.logger.experiment.add_scalars('loss/overview',{'valid_loss': avg_loss}, self.global_step)\n",
    "\n",
    "        tqdm_dict = {'valid_psnr': avg_psnr, 'valid_loss': avg_loss}\n",
    "        return {'progress_bar': tqdm_dict} #, 'log': tqdm_dict}\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "\n",
    "    @pl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        mask_generator = MaskGenerator(self.hparams.mask_dir, self.hparams.height, self.hparams.width, invert_mask=self.hparams.invert_mask) \n",
    "        dataset = ImagesDataset(self.hparams.train_dir, self.hparams.height, self.hparams.width, mask_generator)\n",
    "        dataloader = DataLoader(dataset, batch_size=self.hparams.batch_size, shuffle=True, num_workers=self.hparams.num_workers)\n",
    "        return dataloader\n",
    "    \n",
    "    @pl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        mask_generator = MaskGenerator(self.hparams.mask_dir, self.hparams.height, self.hparams.width, invert_mask=self.hparams.invert_mask) \n",
    "        dataset = ImagesDataset(self.hparams.valid_dir, self.hparams.height, self.hparams.width, mask_generator)\n",
    "        dataloader = DataLoader(dataset, batch_size=self.hparams.batch_size, shuffle=False, num_workers=self.hparams.num_workers)\n",
    "        return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ImageInpaintingSystem(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu available: True, used: True\n",
      "VISIBLE GPUS: 0\n",
      "55116 masks found: ../../../Repos/image-inpainting/dataset/irregular_mask/irregular_mask/disocclusion_img_mask/\n",
      "55116 masks found: ../../../Repos/image-inpainting/dataset/irregular_mask/irregular_mask/disocclusion_img_mask/\n",
      "                               Name           Type Params\n",
      "0                         pConvUNet      PConvUNet   32 M\n",
      "1                pConvUNet.encoder1   PConvEncoder    9 K\n",
      "2          pConvUNet.encoder1.pconv  PartialConv2d    9 K\n",
      "3      pConvUNet.encoder1.batchnorm    BatchNorm2d  128  \n",
      "4     pConvUNet.encoder1.activation           ReLU    0  \n",
      "..                              ...            ...    ...\n",
      "99   vgg16extractor.max_pooling3.12         Conv2d  590 K\n",
      "100  vgg16extractor.max_pooling3.13           ReLU    0  \n",
      "101  vgg16extractor.max_pooling3.14         Conv2d  590 K\n",
      "102  vgg16extractor.max_pooling3.15           ReLU    0  \n",
      "103  vgg16extractor.max_pooling3.16      MaxPool2d    0  \n",
      "\n",
      "[104 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]D:\\NewRepos\\Image-Inpainting\\loss\\loss_compute.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  dilated_mask = torch.tensor(dilated_mask> 0, dtype=torch.float, requires_grad=False).to(self.device)\n",
      "  8%|â–Š         | 657/7828 [06:32<1:12:42,  1.64it/s, batch_nb=655, epoch=0, gpu=0, loss=10.194, train_loss=13.8, v_nb=0, valid_loss=12.3, valid_psnr=7.64]"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "        gpus=1,\n",
    "        train_percent_check=0.1, \n",
    "        val_check_interval=0.05,\n",
    "        use_amp=False,\n",
    "        default_save_path='test_logs2'\n",
    "    )\n",
    "\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# trainer = Trainer(gpus=1, fast_dev_run=True)\n",
    "# trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
